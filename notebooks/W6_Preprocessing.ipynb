{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Extraction\n",
    "\n",
    "We want to put data into machine learning models.\n",
    "\n",
    "But: Data might not be given in a format that can directly be given to our models. (E.g., how do you represent text? How do we transform the text into \"numbers\" that we can feed to our models?)\n",
    "\n",
    "In general, we need to convert our data items (words, sentences, images, videos) into vectors or matrices (or sequences of them).\n",
    "\n",
    "\n",
    "#### Example &ndash; Sentiment dataset\n",
    "\n",
    "Today we will use the [Sentiment Polarity Dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/). It contains 5331 positive and 5331 negative sentences from movie reviews, separated into two files (the positive reviews in one file, and the negative reviews in another one). Download the data from [here](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz), unpack it, and put it in the same folder as this Jupyter Notebook.\n",
    "\n",
    "(I'm probably not supposed to redistribute the data, but just in case anyone has problems to deal with .tar.gz files, I also left in the Downloads Folder a .zip file containing the exact same data)\n",
    "\n",
    "Let's take a look at how the data looks like, for both the positive and negative reviews..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's load some libraries that we will use in the code below\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "--\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "--\n",
      "effective but too-tepid biopic\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Opens file of positive polarity data\n",
    "with open('rt-polaritydata/rt-polaritydata/rt-polarity.pos', \n",
    "          'r', encoding='latin-1') as f:\n",
    "    positives = [sentence.strip() for sentence in f]\n",
    "\n",
    "# Prints the first 3 sentences, separated by \"--\"\n",
    "for i in [0,1,2]:\n",
    "    print(positives[i])\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplistic , silly and tedious .\n",
      "--\n",
      "it's so laddish and juvenile , only teenage boys could possibly find it funny .\n",
      "--\n",
      "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Let's do the same with the negative polarity data\n",
    "with open('rt-polaritydata/rt-polaritydata/rt-polarity.neg',\n",
    "          'r', encoding='latin-1') as f:\n",
    "    negatives = [sentence.strip() for sentence in f]\n",
    "\n",
    "# Let's also print the first 3 sentences\n",
    "for i in [0,1,2]:\n",
    "    print(negatives[i])\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing in NLP\n",
    "\n",
    "We generally need to convert sentences to sequences of manageable \"concepts\" (e.g. words, stems, lemmas).\n",
    "\n",
    "This can include:\n",
    "- Tokenization\n",
    "- Filtering\n",
    "- Lemmatization\n",
    "- POS tagging\n",
    "- Lower case or other transformations\n",
    "- Vocabulary building\n",
    "- Restructuring the data\n",
    "\n",
    "What exactly we do here depends on the method we want to use for processing the data afterwards.\n",
    "\n",
    "#### Example &ndash; Sentiment dataset\n",
    "\n",
    "Coming back to the sentiment dataset, let's select a random sentence from the dataset and illustrate how we would preprocess the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whether writer-director anne fontaine's film is a ghost story , an account of a nervous breakdown , a trip down memory lane , all three or none of the above , it is as seductive as it is haunting .\n"
     ]
    }
   ],
   "source": [
    "# Load a random sentence\n",
    "# (notice that `positives + negatives` is the concatenation of the two lists)\n",
    "example_sentence = random.choice(positives + negatives)\n",
    "\n",
    "# Let's print the chosen sentence\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whether', 'writer-director', 'anne', 'fontaine', \"'s\", 'film', 'is', 'a', 'ghost', 'story', ',', 'an', 'account', 'of', 'a', 'nervous', 'breakdown', ',', 'a', 'trip', 'down', 'memory', 'lane', ',', 'all', 'three', 'or', 'none', 'of', 'the', 'above', ',', 'it', 'is', 'as', 'seductive', 'as', 'it', 'is', 'haunting', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "print(tokenize(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whether', 'anne', 'fontaine', 'film', 'is', 'a', 'ghost', 'story', 'an', 'account', 'of', 'a', 'nervous', 'breakdown', 'a', 'trip', 'down', 'memory', 'lane', 'all', 'three', 'or', 'none', 'of', 'the', 'above', 'it', 'is', 'as', 'seductive', 'as', 'it', 'is', 'haunting']\n"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "def filter_(tokenized_sentence):\n",
    "    # The regular expression will filter out anything that\n",
    "    # is not composed of letters (i.e., contains \",\" or \".\",\n",
    "    # or \"&\", or \"?\", etc.)\n",
    "    return [word for word in tokenized_sentence\n",
    "                  if re.match(r'^[\\w]+$', word)]\n",
    "\n",
    "print(filter_(tokenize(example_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'destined',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " '21st',\n",
       " 'century',\n",
       " 'new',\n",
       " 'conan',\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " 'going',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'splash',\n",
       " 'even',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'arnold',\n",
       " 'schwarzenegger',\n",
       " 'van',\n",
       " 'damme',\n",
       " 'or',\n",
       " 'steven',\n",
       " 'segal',\n",
       " 'the',\n",
       " 'gorgeously',\n",
       " 'elaborate',\n",
       " 'continuation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lord',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rings',\n",
       " 'trilogy',\n",
       " 'is',\n",
       " 'so',\n",
       " 'huge',\n",
       " 'that',\n",
       " 'a',\n",
       " 'column',\n",
       " 'of',\n",
       " 'words',\n",
       " 'can',\n",
       " 'not',\n",
       " 'adequately',\n",
       " 'describe',\n",
       " 'peter',\n",
       " 'jackson',\n",
       " 'expanded',\n",
       " 'vision',\n",
       " 'of',\n",
       " 'j',\n",
       " 'r',\n",
       " 'r',\n",
       " 'tolkien',\n",
       " 'effective',\n",
       " 'but',\n",
       " 'biopic',\n",
       " 'if',\n",
       " 'you',\n",
       " 'sometimes',\n",
       " 'like',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'to',\n",
       " 'have',\n",
       " 'fun',\n",
       " 'wasabi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'start',\n",
       " 'emerges',\n",
       " 'as',\n",
       " 'something',\n",
       " 'rare',\n",
       " 'an',\n",
       " 'issue',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'so',\n",
       " 'honest',\n",
       " 'and',\n",
       " 'keenly',\n",
       " 'observed',\n",
       " 'that',\n",
       " 'it',\n",
       " 'does',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'one',\n",
       " 'the',\n",
       " 'film',\n",
       " 'provides',\n",
       " 'some',\n",
       " 'great',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'the',\n",
       " 'neurotic',\n",
       " 'mindset',\n",
       " 'of',\n",
       " 'all',\n",
       " 'comics',\n",
       " 'even',\n",
       " 'those',\n",
       " 'who',\n",
       " 'have',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'absolute',\n",
       " 'top',\n",
       " 'of',\n",
       " 'the',\n",
       " 'game',\n",
       " 'offers',\n",
       " 'that',\n",
       " 'rare',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'entertainment',\n",
       " 'and',\n",
       " 'education',\n",
       " 'perhaps',\n",
       " 'no',\n",
       " 'picture',\n",
       " 'ever',\n",
       " 'made',\n",
       " 'has',\n",
       " 'more',\n",
       " 'literally',\n",
       " 'showed',\n",
       " 'that',\n",
       " 'the',\n",
       " 'road',\n",
       " 'to',\n",
       " 'hell',\n",
       " 'is',\n",
       " 'paved',\n",
       " 'with',\n",
       " 'good',\n",
       " 'intentions',\n",
       " 'steers',\n",
       " 'turns',\n",
       " 'in',\n",
       " 'a',\n",
       " 'snappy',\n",
       " 'screenplay',\n",
       " 'that',\n",
       " 'curls',\n",
       " 'at',\n",
       " 'the',\n",
       " 'edges',\n",
       " 'it',\n",
       " 'so',\n",
       " 'clever',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hate',\n",
       " 'it',\n",
       " 'but',\n",
       " 'he',\n",
       " 'somehow',\n",
       " 'pulls',\n",
       " 'it',\n",
       " 'off',\n",
       " 'take',\n",
       " 'care',\n",
       " 'of',\n",
       " 'my',\n",
       " 'cat',\n",
       " 'offers',\n",
       " 'a',\n",
       " 'refreshingly',\n",
       " 'different',\n",
       " 'slice',\n",
       " 'of',\n",
       " 'asian',\n",
       " 'cinema',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'film',\n",
       " 'well',\n",
       " 'worth',\n",
       " 'seeing',\n",
       " 'talking',\n",
       " 'and',\n",
       " 'singing',\n",
       " 'heads',\n",
       " 'and',\n",
       " 'all',\n",
       " 'what',\n",
       " 'really',\n",
       " 'surprises',\n",
       " 'about',\n",
       " 'wisegirls',\n",
       " 'is',\n",
       " 'its',\n",
       " 'quality',\n",
       " 'and',\n",
       " 'genuine',\n",
       " 'tenderness',\n",
       " 'wendigo',\n",
       " 'is',\n",
       " 'why',\n",
       " 'we',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'cinema',\n",
       " 'to',\n",
       " 'be',\n",
       " 'fed',\n",
       " 'through',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'the',\n",
       " 'mind',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'movies',\n",
       " 'ever',\n",
       " 'ultimately',\n",
       " 'it',\n",
       " 'ponders',\n",
       " 'the',\n",
       " 'reasons',\n",
       " 'we',\n",
       " 'need',\n",
       " 'stories',\n",
       " 'so',\n",
       " 'much',\n",
       " 'an',\n",
       " 'utterly',\n",
       " 'compelling',\n",
       " 'wrote',\n",
       " 'it',\n",
       " 'in',\n",
       " 'which',\n",
       " 'the',\n",
       " 'reputation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'famous',\n",
       " 'author',\n",
       " 'who',\n",
       " 'ever',\n",
       " 'lived',\n",
       " 'comes',\n",
       " 'into',\n",
       " 'question',\n",
       " 'illuminating',\n",
       " 'if',\n",
       " 'overly',\n",
       " 'talky',\n",
       " 'documentary',\n",
       " 'a',\n",
       " 'masterpiece',\n",
       " 'four',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'making',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'ripe',\n",
       " 'enrapturing',\n",
       " 'beauty',\n",
       " 'will',\n",
       " 'tempt',\n",
       " 'those',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'probe',\n",
       " 'its',\n",
       " 'inscrutable',\n",
       " 'mysteries',\n",
       " 'offers',\n",
       " 'a',\n",
       " 'breath',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fresh',\n",
       " 'air',\n",
       " 'of',\n",
       " 'true',\n",
       " 'sophistication',\n",
       " 'a',\n",
       " 'thoughtful',\n",
       " 'provocative',\n",
       " 'insistently',\n",
       " 'humanizing',\n",
       " 'film',\n",
       " 'with',\n",
       " 'a',\n",
       " 'cast',\n",
       " 'that',\n",
       " 'includes',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'top',\n",
       " 'actors',\n",
       " 'working',\n",
       " 'in',\n",
       " 'independent',\n",
       " 'film',\n",
       " 'lovely',\n",
       " 'amazing',\n",
       " 'involves',\n",
       " 'us',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'so',\n",
       " 'incisive',\n",
       " 'so',\n",
       " 'bleakly',\n",
       " 'amusing',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'go',\n",
       " 'about',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'a',\n",
       " 'disturbing',\n",
       " 'and',\n",
       " 'frighteningly',\n",
       " 'evocative',\n",
       " 'assembly',\n",
       " 'of',\n",
       " 'imagery',\n",
       " 'and',\n",
       " 'hypnotic',\n",
       " 'music',\n",
       " 'composed',\n",
       " 'by',\n",
       " 'philip',\n",
       " 'glass',\n",
       " 'not',\n",
       " 'for',\n",
       " 'everyone',\n",
       " 'but',\n",
       " 'for',\n",
       " 'those',\n",
       " 'with',\n",
       " 'whom',\n",
       " 'it',\n",
       " 'will',\n",
       " 'connect',\n",
       " 'it',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'departure',\n",
       " 'from',\n",
       " 'standard',\n",
       " 'moviegoing',\n",
       " 'fare',\n",
       " 'scores',\n",
       " 'a',\n",
       " 'few',\n",
       " 'points',\n",
       " 'for',\n",
       " 'doing',\n",
       " 'what',\n",
       " 'it',\n",
       " 'does',\n",
       " 'with',\n",
       " 'a',\n",
       " 'dedicated',\n",
       " 'and',\n",
       " 'professionalism',\n",
       " 'occasionally',\n",
       " 'melodramatic',\n",
       " 'it',\n",
       " 'also',\n",
       " 'extremely',\n",
       " 'effective',\n",
       " 'spiderman',\n",
       " 'rocks',\n",
       " 'an',\n",
       " 'idealistic',\n",
       " 'love',\n",
       " 'story',\n",
       " 'that',\n",
       " 'brings',\n",
       " 'out',\n",
       " 'the',\n",
       " 'latent',\n",
       " 'romantic',\n",
       " 'in',\n",
       " 'everyone',\n",
       " 'at',\n",
       " 'about',\n",
       " '95',\n",
       " 'minutes',\n",
       " 'treasure',\n",
       " 'planet',\n",
       " 'maintains',\n",
       " 'a',\n",
       " 'brisk',\n",
       " 'pace',\n",
       " 'as',\n",
       " 'it',\n",
       " 'races',\n",
       " 'through',\n",
       " 'the',\n",
       " 'familiar',\n",
       " 'story',\n",
       " 'however',\n",
       " 'it',\n",
       " 'lacks',\n",
       " 'grandeur',\n",
       " 'and',\n",
       " 'that',\n",
       " 'epic',\n",
       " 'quality',\n",
       " 'often',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'stevenson',\n",
       " 'tale',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'with',\n",
       " 'earlier',\n",
       " 'disney',\n",
       " 'efforts',\n",
       " 'it',\n",
       " 'helps',\n",
       " 'that',\n",
       " 'lil',\n",
       " 'bow',\n",
       " 'wow',\n",
       " 'tones',\n",
       " 'down',\n",
       " 'his',\n",
       " 'gangsta',\n",
       " 'act',\n",
       " 'to',\n",
       " 'play',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'resembles',\n",
       " 'a',\n",
       " 'real',\n",
       " 'kid',\n",
       " 'guaranteed',\n",
       " 'to',\n",
       " 'move',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'ever',\n",
       " 'shook',\n",
       " 'rattled',\n",
       " 'or',\n",
       " 'rolled',\n",
       " 'a',\n",
       " 'masterful',\n",
       " 'film',\n",
       " 'from',\n",
       " 'a',\n",
       " 'master',\n",
       " 'filmmaker',\n",
       " 'unique',\n",
       " 'in',\n",
       " 'its',\n",
       " 'deceptive',\n",
       " 'grimness',\n",
       " 'compelling',\n",
       " 'in',\n",
       " 'its',\n",
       " 'fatalist',\n",
       " 'worldview',\n",
       " 'light',\n",
       " 'cute',\n",
       " 'and',\n",
       " 'forgettable',\n",
       " 'if',\n",
       " 'there',\n",
       " 'a',\n",
       " 'way',\n",
       " 'to',\n",
       " 'effectively',\n",
       " 'teach',\n",
       " 'kids',\n",
       " 'about',\n",
       " 'the',\n",
       " 'dangers',\n",
       " 'of',\n",
       " 'drugs',\n",
       " 'i',\n",
       " 'think',\n",
       " 'it',\n",
       " 'in',\n",
       " 'projects',\n",
       " 'like',\n",
       " 'the',\n",
       " 'unfortunately',\n",
       " 'paid',\n",
       " 'while',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'give',\n",
       " 'crush',\n",
       " 'the',\n",
       " 'new',\n",
       " 'title',\n",
       " 'of',\n",
       " 'two',\n",
       " 'weddings',\n",
       " 'and',\n",
       " 'a',\n",
       " 'funeral',\n",
       " 'it',\n",
       " 'a',\n",
       " 'far',\n",
       " 'more',\n",
       " 'thoughtful',\n",
       " 'film',\n",
       " 'than',\n",
       " 'any',\n",
       " 'slice',\n",
       " 'of',\n",
       " 'hugh',\n",
       " 'grant',\n",
       " 'whimsy',\n",
       " 'though',\n",
       " 'everything',\n",
       " 'might',\n",
       " 'be',\n",
       " 'literate',\n",
       " 'and',\n",
       " 'smart',\n",
       " 'it',\n",
       " 'never',\n",
       " 'took',\n",
       " 'off',\n",
       " 'and',\n",
       " 'always',\n",
       " 'seemed',\n",
       " 'static',\n",
       " 'cantet',\n",
       " 'perfectly',\n",
       " 'captures',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'lobbies',\n",
       " 'highways',\n",
       " 'and',\n",
       " 'roadside',\n",
       " 'cafes',\n",
       " 'that',\n",
       " 'permeate',\n",
       " 'vincent',\n",
       " 'days',\n",
       " 'ms',\n",
       " 'is',\n",
       " 'almost',\n",
       " 'spooky',\n",
       " 'in',\n",
       " 'her',\n",
       " 'sulky',\n",
       " 'calculating',\n",
       " 'lolita',\n",
       " 'turn',\n",
       " 'though',\n",
       " 'it',\n",
       " 'is',\n",
       " 'by',\n",
       " 'no',\n",
       " 'means',\n",
       " 'his',\n",
       " 'best',\n",
       " 'work',\n",
       " 'is',\n",
       " 'a',\n",
       " 'distinguished',\n",
       " 'and',\n",
       " 'distinctive',\n",
       " 'effort',\n",
       " 'by',\n",
       " 'a',\n",
       " 'master',\n",
       " 'a',\n",
       " 'fascinating',\n",
       " 'film',\n",
       " 'replete',\n",
       " 'with',\n",
       " 'rewards',\n",
       " 'to',\n",
       " 'be',\n",
       " 'had',\n",
       " 'by',\n",
       " 'all',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'reap',\n",
       " 'them',\n",
       " 'like',\n",
       " 'most',\n",
       " 'bond',\n",
       " 'outings',\n",
       " 'in',\n",
       " 'recent',\n",
       " 'years',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stunts',\n",
       " 'are',\n",
       " 'so',\n",
       " 'outlandish',\n",
       " 'that',\n",
       " 'they',\n",
       " 'border',\n",
       " 'on',\n",
       " 'being',\n",
       " 'cartoonlike',\n",
       " 'a',\n",
       " 'heavy',\n",
       " 'reliance',\n",
       " 'on',\n",
       " 'cgi',\n",
       " 'technology',\n",
       " 'is',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'creep',\n",
       " 'into',\n",
       " 'the',\n",
       " 'series',\n",
       " 'newton',\n",
       " 'draws',\n",
       " 'our',\n",
       " 'attention',\n",
       " 'like',\n",
       " 'a',\n",
       " 'magnet',\n",
       " 'and',\n",
       " 'acts',\n",
       " 'circles',\n",
       " 'around',\n",
       " 'her',\n",
       " 'better',\n",
       " 'known',\n",
       " 'mark',\n",
       " 'wahlberg',\n",
       " 'the',\n",
       " 'story',\n",
       " 'loses',\n",
       " 'its',\n",
       " 'bite',\n",
       " 'in',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'that',\n",
       " 'even',\n",
       " 'less',\n",
       " 'plausible',\n",
       " 'than',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'much',\n",
       " 'of',\n",
       " 'the',\n",
       " 'way',\n",
       " 'though',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'refreshingly',\n",
       " 'novel',\n",
       " 'ride',\n",
       " 'fuller',\n",
       " 'would',\n",
       " 'surely',\n",
       " 'have',\n",
       " 'called',\n",
       " 'this',\n",
       " 'gutsy',\n",
       " 'and',\n",
       " 'at',\n",
       " 'times',\n",
       " 'exhilarating',\n",
       " 'movie',\n",
       " 'a',\n",
       " 'great',\n",
       " 'yarn',\n",
       " 'e',\n",
       " 'intelectualmente',\n",
       " 'retadora',\n",
       " 'el',\n",
       " 'ladrón',\n",
       " 'de',\n",
       " 'orquídeas',\n",
       " 'es',\n",
       " 'uno',\n",
       " 'de',\n",
       " 'esos',\n",
       " 'filmes',\n",
       " 'que',\n",
       " 'vale',\n",
       " 'la',\n",
       " 'pena',\n",
       " 'ver',\n",
       " 'precisamente',\n",
       " 'por',\n",
       " 'su',\n",
       " 'originalidad',\n",
       " 'the',\n",
       " 'film',\n",
       " 'makes',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'case',\n",
       " 'for',\n",
       " 'the',\n",
       " 'importance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'musicians',\n",
       " 'in',\n",
       " 'creating',\n",
       " 'the',\n",
       " 'motown',\n",
       " 'sound',\n",
       " 'karmen',\n",
       " 'moves',\n",
       " 'like',\n",
       " 'rhythm',\n",
       " 'itself',\n",
       " 'her',\n",
       " 'lips',\n",
       " 'chanting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'beat',\n",
       " 'her',\n",
       " 'long',\n",
       " 'braided',\n",
       " 'hair',\n",
       " 'doing',\n",
       " 'little',\n",
       " 'to',\n",
       " 'wipe',\n",
       " 'away',\n",
       " 'the',\n",
       " 'jeweled',\n",
       " 'beads',\n",
       " 'of',\n",
       " 'sweat',\n",
       " 'gosling',\n",
       " 'provides',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'performance',\n",
       " 'that',\n",
       " 'dwarfs',\n",
       " 'everything',\n",
       " 'else',\n",
       " 'in',\n",
       " 'the',\n",
       " 'film',\n",
       " 'a',\n",
       " 'real',\n",
       " 'movie',\n",
       " 'about',\n",
       " 'real',\n",
       " 'people',\n",
       " 'that',\n",
       " 'gives',\n",
       " 'us',\n",
       " 'a',\n",
       " 'rare',\n",
       " 'glimpse',\n",
       " 'into',\n",
       " 'a',\n",
       " 'culture',\n",
       " 'most',\n",
       " 'of',\n",
       " 'us',\n",
       " 'do',\n",
       " 'know',\n",
       " 'tender',\n",
       " 'yet',\n",
       " 'lacerating',\n",
       " 'and',\n",
       " 'darkly',\n",
       " 'funny',\n",
       " 'fable',\n",
       " 'may',\n",
       " 'be',\n",
       " 'spoofing',\n",
       " 'an',\n",
       " 'easy',\n",
       " 'target',\n",
       " 'those',\n",
       " 'old',\n",
       " 'giant',\n",
       " 'creature',\n",
       " 'features',\n",
       " 'but',\n",
       " 'it',\n",
       " 'acknowledges',\n",
       " 'and',\n",
       " 'celebrates',\n",
       " 'their',\n",
       " 'cheesiness',\n",
       " 'as',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'people',\n",
       " 'get',\n",
       " 'a',\n",
       " 'kick',\n",
       " 'out',\n",
       " 'of',\n",
       " 'watching',\n",
       " 'them',\n",
       " 'today',\n",
       " 'an',\n",
       " 'engaging',\n",
       " 'overview',\n",
       " 'of',\n",
       " 'johnson',\n",
       " 'eccentric',\n",
       " 'career',\n",
       " 'in',\n",
       " 'its',\n",
       " 'ragged',\n",
       " 'cheap',\n",
       " 'and',\n",
       " 'unassuming',\n",
       " 'way',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'works',\n",
       " 'some',\n",
       " 'actors',\n",
       " 'have',\n",
       " 'so',\n",
       " 'much',\n",
       " 'charisma',\n",
       " 'that',\n",
       " 'you',\n",
       " 'be',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'them',\n",
       " 'reading',\n",
       " 'the',\n",
       " 'phone',\n",
       " 'book',\n",
       " 'hugh',\n",
       " 'grant',\n",
       " 'and',\n",
       " 'sandra',\n",
       " 'bullock',\n",
       " 'are',\n",
       " 'two',\n",
       " 'such',\n",
       " 'likeable',\n",
       " 'actors',\n",
       " 'sandra',\n",
       " 'nettelbeck',\n",
       " 'beautifully',\n",
       " 'orchestrates',\n",
       " 'the',\n",
       " 'transformation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chilly',\n",
       " 'neurotic',\n",
       " 'and',\n",
       " 'martha',\n",
       " 'as',\n",
       " 'her',\n",
       " 'heart',\n",
       " 'begins',\n",
       " 'to',\n",
       " 'open',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'snow',\n",
       " 'games',\n",
       " 'and',\n",
       " 'lovable',\n",
       " 'siberian',\n",
       " 'huskies',\n",
       " 'plus',\n",
       " 'one',\n",
       " 'sheep',\n",
       " 'dog',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'hosts',\n",
       " 'a',\n",
       " 'dose',\n",
       " 'of',\n",
       " 'heart',\n",
       " 'everytime',\n",
       " 'you',\n",
       " 'think',\n",
       " 'undercover',\n",
       " 'brother',\n",
       " 'has',\n",
       " 'run',\n",
       " 'out',\n",
       " 'of',\n",
       " 'steam',\n",
       " 'it',\n",
       " 'finds',\n",
       " 'a',\n",
       " 'new',\n",
       " 'way',\n",
       " 'to',\n",
       " 'surprise',\n",
       " 'and',\n",
       " 'amuse',\n",
       " 'manages',\n",
       " 'to',\n",
       " 'be',\n",
       " 'original',\n",
       " 'even',\n",
       " 'though',\n",
       " 'it',\n",
       " 'rips',\n",
       " 'off',\n",
       " 'many',\n",
       " 'of',\n",
       " 'its',\n",
       " 'ideas',\n",
       " 'bryan',\n",
       " 'adams',\n",
       " 'contributes',\n",
       " 'a',\n",
       " 'slew',\n",
       " 'of',\n",
       " 'songs',\n",
       " 'a',\n",
       " 'few',\n",
       " 'potential',\n",
       " 'hits',\n",
       " 'a',\n",
       " 'few',\n",
       " 'more',\n",
       " 'simply',\n",
       " 'intrusive',\n",
       " 'to',\n",
       " 'the',\n",
       " 'story',\n",
       " 'but',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'package',\n",
       " 'certainly',\n",
       " 'captures',\n",
       " 'the',\n",
       " 'intended',\n",
       " 'er',\n",
       " 'spirit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'piece',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Of course, we can do the same with all words in our dataset. For example...\n",
    "[word for sent in positives+negatives\n",
    "      for word in filter_(tokenize(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And if we want to know the most frequent words, we already know what to use\n",
    "words = nltk.FreqDist([word for sent in positives for word in filter_(tokenize(sent))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5060),\n",
       " ('a', 3845),\n",
       " ('and', 3554),\n",
       " ('of', 3312),\n",
       " ('to', 1970),\n",
       " ('is', 1776),\n",
       " ('it', 1674),\n",
       " ('that', 1357),\n",
       " ('in', 1340),\n",
       " ('film', 899),\n",
       " ('with', 884),\n",
       " ('as', 880),\n",
       " ('but', 784),\n",
       " ('an', 758),\n",
       " ('its', 700),\n",
       " ('for', 674),\n",
       " ('this', 667),\n",
       " ('you', 636),\n",
       " ('movie', 538),\n",
       " ('on', 424)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'it',\n",
       " 'that',\n",
       " 'in',\n",
       " 'film',\n",
       " 'with',\n",
       " 'as',\n",
       " 'but',\n",
       " 'an',\n",
       " 'its',\n",
       " 'for',\n",
       " 'this',\n",
       " 'you',\n",
       " 'movie',\n",
       " 'on']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want only the words, without the frequency, we can just take\n",
    "# the first element of the tuple\n",
    "[i[0] for i in words.most_common(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our preprocessing, we will want to create a \"vocabulary\". This vocabulary will be typically composed of the most common words in our text. The size of this vocabulary is arbitrary, and depends on things like our application, the amount of data we have, or the amount of resources our application is supposed to use.\n",
    "\n",
    "We already know how to take the most common words. Now we need only two other things:\n",
    "\n",
    " * A symbol for the \"other words\", not the most common;\n",
    " * A numerical ID for each word.\n",
    "\n",
    "Let's put this all into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives\n",
    "#  * A set of sentences\n",
    "#  * A vocabulary size\n",
    "#  * The \"Unknown Symbol\"\n",
    "# Returns\n",
    "#  * A set of words\n",
    "#  * A dictionary mapping each word to an ID\n",
    "def build_vocabulary(sentences, vocabulary_size, unknown_symbol):\n",
    "    \n",
    "    # We already ran this part above\n",
    "    words = nltk.FreqDist([word for sent in sentences for word in filter_(tokenize(sent))])\n",
    "    \n",
    "    # Creates a list with all the words.\n",
    "    vocabulary = [item[0] for item in words.most_common(vocabulary_size)]\n",
    "    \n",
    "    # Inserts the `unknown_symbol` in the first position\n",
    "    vocabulary = [unknown_symbol] + vocabulary\n",
    "    \n",
    "    # Creates a mapping \"word --> index\"\n",
    "    word_to_idx = {word: index for index, word in enumerate(vocabulary)}\n",
    "\n",
    "    # Returns a set of words and the dictionary\n",
    "    return set(vocabulary), word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now think again what we have achieved. Given the dataset, we created a function that is now able to get the most common words of this dataset and put it in a data structure (our \"vocabulary\").\n",
    "\n",
    "I understand that this function may be still a little hard to understand. Maybe it will become a little clearer if we take a look at how this vocabulary looks like. Let's apply this function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all sentences\n",
    "all_sents = positives+negatives\n",
    "\n",
    "# Calls `build_vocabulary` on all sentences\n",
    "vocabulary_set, word_to_idx = build_vocabulary(sentences=all_sents,\n",
    "                                               vocabulary_size=200,\n",
    "                                               unknown_symbol=\"<UNK>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the two things that the function returned. The first element, called `vocabulary_set`, is a `set` containing each one of the words in the vocabulary. In `build_vocabulary()`, we chose these to be the most common words in the dataset (we passed `vocabulary_size=200` to choose only the 200 most common words). Let's take a look at these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>',\n",
       " 'a',\n",
       " 'about',\n",
       " 'action',\n",
       " 'after',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'american',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'audience',\n",
       " 'bad',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'being',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'can',\n",
       " 'cast',\n",
       " 'character',\n",
       " 'characters',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'de',\n",
       " 'despite',\n",
       " 'director',\n",
       " 'do',\n",
       " 'documentary',\n",
       " 'does',\n",
       " 'down',\n",
       " 'drama',\n",
       " 'end',\n",
       " 'enough',\n",
       " 'entertaining',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'family',\n",
       " 'far',\n",
       " 'feel',\n",
       " 'feels',\n",
       " 'few',\n",
       " 'film',\n",
       " 'films',\n",
       " 'first',\n",
       " 'for',\n",
       " 'from',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'get',\n",
       " 'go',\n",
       " 'good',\n",
       " 'great',\n",
       " 'had',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'heart',\n",
       " 'her',\n",
       " 'here',\n",
       " 'his',\n",
       " 'hollywood',\n",
       " 'how',\n",
       " 'humor',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'interesting',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'kind',\n",
       " 'less',\n",
       " 'life',\n",
       " 'like',\n",
       " 'little',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'made',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'man',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'might',\n",
       " 'minutes',\n",
       " 'moments',\n",
       " 'more',\n",
       " 'most',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'much',\n",
       " 'my',\n",
       " 'never',\n",
       " 'new',\n",
       " 'no',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'one',\n",
       " 'only',\n",
       " 'or',\n",
       " 'original',\n",
       " 'other',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'people',\n",
       " 'performance',\n",
       " 'performances',\n",
       " 'picture',\n",
       " 'plot',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 'real',\n",
       " 'really',\n",
       " 'romantic',\n",
       " 'screen',\n",
       " 'script',\n",
       " 'see',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'sense',\n",
       " 'should',\n",
       " 'so',\n",
       " 'some',\n",
       " 'something',\n",
       " 'still',\n",
       " 'story',\n",
       " 'such',\n",
       " 'take',\n",
       " 'tale',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thriller',\n",
       " 'through',\n",
       " 'time',\n",
       " 'to',\n",
       " 'too',\n",
       " 'two',\n",
       " 'ultimately',\n",
       " 'up',\n",
       " 'us',\n",
       " 'very',\n",
       " 'was',\n",
       " 'watching',\n",
       " 'way',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'will',\n",
       " 'with',\n",
       " 'without',\n",
       " 'work',\n",
       " 'world',\n",
       " 'would',\n",
       " 'year',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see that `'<UNK>'` is a word in the vocabulary. All the \"rest\" of the words, i.e., those that were not among the 200 most common, were grouped together into the \"UNK\" character. We are basically saying we don't care about those other words. They are probably not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the other data structure output by the vocabulary: the `word_to_idx` dictionary. It has a mapping word-to-number, where each word in our dictionary gets assigned a different number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'the': 1,\n",
       " 'a': 2,\n",
       " 'and': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'it': 7,\n",
       " 'that': 8,\n",
       " 'in': 9,\n",
       " 'as': 10,\n",
       " 'but': 11,\n",
       " 'film': 12,\n",
       " 'with': 13,\n",
       " 'this': 14,\n",
       " 'for': 15,\n",
       " 'its': 16,\n",
       " 'movie': 17,\n",
       " 'an': 18,\n",
       " 'you': 19,\n",
       " 'be': 20,\n",
       " 'on': 21,\n",
       " 'not': 22,\n",
       " 'by': 23,\n",
       " 'one': 24,\n",
       " 'about': 25,\n",
       " 'are': 26,\n",
       " 'has': 27,\n",
       " 'more': 28,\n",
       " 'like': 29,\n",
       " 'at': 30,\n",
       " 'from': 31,\n",
       " 'than': 32,\n",
       " 'all': 33,\n",
       " 'have': 34,\n",
       " 'his': 35,\n",
       " 'i': 36,\n",
       " 'so': 37,\n",
       " 'if': 38,\n",
       " 'or': 39,\n",
       " 'story': 40,\n",
       " 'what': 41,\n",
       " 'there': 42,\n",
       " 'too': 43,\n",
       " 'who': 44,\n",
       " 'just': 45,\n",
       " 'does': 46,\n",
       " 'into': 47,\n",
       " 'most': 48,\n",
       " 'out': 49,\n",
       " 'no': 50,\n",
       " 'much': 51,\n",
       " 'even': 52,\n",
       " 'good': 53,\n",
       " 'up': 54,\n",
       " 'will': 55,\n",
       " 'can': 56,\n",
       " 'comedy': 57,\n",
       " 'time': 58,\n",
       " 'some': 59,\n",
       " 'characters': 60,\n",
       " 'he': 61,\n",
       " 'only': 62,\n",
       " 'little': 63,\n",
       " 'way': 64,\n",
       " 'their': 65,\n",
       " 'do': 66,\n",
       " 'funny': 67,\n",
       " 'make': 68,\n",
       " 'they': 69,\n",
       " 'enough': 70,\n",
       " 'been': 71,\n",
       " 'very': 72,\n",
       " 'we': 73,\n",
       " 'your': 74,\n",
       " 'never': 75,\n",
       " 'when': 76,\n",
       " 'director': 77,\n",
       " 'makes': 78,\n",
       " 'would': 79,\n",
       " 'may': 80,\n",
       " 'which': 81,\n",
       " 'us': 82,\n",
       " 'work': 83,\n",
       " 'best': 84,\n",
       " 'was': 85,\n",
       " 'bad': 86,\n",
       " 'life': 87,\n",
       " 'love': 88,\n",
       " 'any': 89,\n",
       " 'could': 90,\n",
       " 'while': 91,\n",
       " 'movies': 92,\n",
       " 'new': 93,\n",
       " 'well': 94,\n",
       " 'her': 95,\n",
       " 'something': 96,\n",
       " 'through': 97,\n",
       " 'really': 98,\n",
       " 'how': 99,\n",
       " 'should': 100,\n",
       " 'made': 101,\n",
       " 'them': 102,\n",
       " 'performances': 103,\n",
       " 'own': 104,\n",
       " 'plot': 105,\n",
       " 'many': 106,\n",
       " 'drama': 107,\n",
       " 'those': 108,\n",
       " 'films': 109,\n",
       " 'still': 110,\n",
       " 'see': 111,\n",
       " 'look': 112,\n",
       " 'every': 113,\n",
       " 'two': 114,\n",
       " 'people': 115,\n",
       " 'nothing': 116,\n",
       " 'better': 117,\n",
       " 'long': 118,\n",
       " 'other': 119,\n",
       " 'without': 120,\n",
       " 'fun': 121,\n",
       " 'off': 122,\n",
       " 'get': 123,\n",
       " 'being': 124,\n",
       " 'action': 125,\n",
       " 'both': 126,\n",
       " 'great': 127,\n",
       " 'though': 128,\n",
       " 'might': 129,\n",
       " 'big': 130,\n",
       " 'also': 131,\n",
       " 'here': 132,\n",
       " 'character': 133,\n",
       " 'audience': 134,\n",
       " 'another': 135,\n",
       " 'cast': 136,\n",
       " 'script': 137,\n",
       " 'humor': 138,\n",
       " 'kind': 139,\n",
       " 'between': 140,\n",
       " 'first': 141,\n",
       " 'sense': 142,\n",
       " 'such': 143,\n",
       " 'over': 144,\n",
       " 'world': 145,\n",
       " 'ever': 146,\n",
       " 'performance': 147,\n",
       " 'feels': 148,\n",
       " 'few': 149,\n",
       " 'thing': 150,\n",
       " 'because': 151,\n",
       " 'far': 152,\n",
       " 'often': 153,\n",
       " 'less': 154,\n",
       " 'seems': 155,\n",
       " 'minutes': 156,\n",
       " 'real': 157,\n",
       " 'picture': 158,\n",
       " 'feel': 159,\n",
       " 'thriller': 160,\n",
       " 'tale': 161,\n",
       " 'almost': 162,\n",
       " 'ca': 163,\n",
       " 'quite': 164,\n",
       " 'documentary': 165,\n",
       " 'down': 166,\n",
       " 'yet': 167,\n",
       " 'interesting': 168,\n",
       " 'these': 169,\n",
       " 'entertaining': 170,\n",
       " 'my': 171,\n",
       " 'screen': 172,\n",
       " 'rather': 173,\n",
       " 'man': 174,\n",
       " 'hollywood': 175,\n",
       " 'were': 176,\n",
       " 'end': 177,\n",
       " 'itself': 178,\n",
       " 'take': 179,\n",
       " 'watching': 180,\n",
       " 'seen': 181,\n",
       " 'full': 182,\n",
       " 'go': 183,\n",
       " 'ultimately': 184,\n",
       " 'de': 185,\n",
       " 'hard': 186,\n",
       " 'heart': 187,\n",
       " 'comes': 188,\n",
       " 'romantic': 189,\n",
       " 'moments': 190,\n",
       " 'lot': 191,\n",
       " 'despite': 192,\n",
       " 'where': 193,\n",
       " 'had': 194,\n",
       " 'american': 195,\n",
       " 'family': 196,\n",
       " 'me': 197,\n",
       " 'after': 198,\n",
       " 'original': 199,\n",
       " 'year': 200}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the \"UNK\" word also got assigned a number. It is common to assign 0 to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['<UNK>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with a vocabulary in hand, we can \"preprocess\" a sentence by transforming each of our words into a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = filter_(tokenize(example_sentence))\n",
    "\n",
    "preprocessed = []\n",
    "for word in filtered_sentence:\n",
    "    if word in vocabulary_set:\n",
    "        preprocessed.append(word_to_idx[word])\n",
    "    else:\n",
    "        preprocessed.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 40,\n",
       " 18,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 166,\n",
       " 0,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 39,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 10,\n",
       " 0,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can use list comprehensions instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 40,\n",
       " 18,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 166,\n",
       " 0,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 39,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 10,\n",
       " 0,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_to_idx[word] if word in vocabulary_set else 0\n",
    "            for word in filtered_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, to avoid having to write this again and again, we can put this code into a function. Given a sentence and a vocabulary, this function transforms the sentence into a sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentence to sequence of integers\n",
    "def preprocess(sentence, word_to_idx, vocabulary_set):\n",
    "    # `else 0` assumes that first vocabulary item is the \"unknown word\"\n",
    "    return [word_to_idx[word] if word in vocabulary_set else 0\n",
    "            for word in filter_(tokenize(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"whether writer-director anne fontaine's film is a ghost story , an account of a nervous breakdown , a trip down memory lane , all three or none of the above , it is as seductive as it is haunting .\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 40,\n",
       " 18,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 166,\n",
       " 0,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 39,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 10,\n",
       " 0,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(example_sentence, word_to_idx, vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "Ok... so now we transformed our words into numbers. Unfortunately, this is often still not enough.\n",
    "\n",
    "For many models, you will still do some sort of _feature extraction_ after preprocessing.\n",
    "\n",
    "Roughly speaking, a _feature_ is some measurable characteristic that you can observe in given items. There are many different examples of features in the context of NLP. For example, for sentences you could extract features like...:\n",
    "\n",
    "- _number of characters_\n",
    "- total number of _words with negative sentiment_ (based on a list of negative words)\n",
    "- counts of a certain _POS-tag_ (e.g., counts of nouns, or of verbs)\n",
    "- _Occurrence count_ of some fixed word\n",
    "\n",
    "A set of such numeric features is then referred to as a _feature vector_. This is what will finally be used as input to machine learning models.\n",
    "\n",
    "One common type of feature vector are the so-called _\"bag-of-words (BoW)\" features_. To generate them, we follow the steps below:\n",
    "\n",
    "1. You first take a vocabulary (typically from the most common words from your dataset). Let's say your vocabulary consists of the words $w_1, \\ldots, w_v$.\n",
    "2. Now, for each sentence,\n",
    "  1. You count how many times each of the vocabulary words occurs in the sentence. Let's say word $w_i$ occurs $n_i$ times in the sentence.\n",
    "  2. The BoW feature vector of the sentence is then given by $(n_1, \\ldots, n_v)$.\n",
    "\n",
    "\n",
    "#### Example &ndash; Sentiment dataset\n",
    "\n",
    "Let's compute bag-of-words representations for sentences of our given dataset.\n",
    "\n",
    "We wrap it all up so we can re-use it in a more compact way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a sentence into a bag of words\n",
    "def bag_of_words(sentence, vocabulary_set, word_to_idx):\n",
    "    \n",
    "    # Generates a vector of zeros\n",
    "    bow_vector = np.zeros(len(vocabulary_set), dtype=\"uint8\")\n",
    "\n",
    "    # Transforms the sentence into a sequence of numbers\n",
    "    indices = preprocess(sentence, word_to_idx, vocabulary_set)\n",
    "    \n",
    "    # Counts the occurrence of each indice\n",
    "    for idx in indices:\n",
    "        bow_vector[idx] += 1\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"whether writer-director anne fontaine's film is a ghost story , an account of a nervous breakdown , a trip down memory lane , all three or none of the above , it is as seductive as it is haunting .\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just recapitulating the example sentence\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  1  3  0  2  0  3  2  0  0  2  0  1  0  0  0  0  0  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words(example_sentence, vocabulary_set, word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an actually nice way of doing it seriously using \"clojures\",\n",
    "# an advanced Python feature that we won't discuss here. I'm leaving it here\n",
    "# in case anyone is interested...\n",
    "#\n",
    "# def feature_function(sentences, vocabulary_size=200):\n",
    "#     \"\"\"This returns a function that takes a sentence and returns a\n",
    "#        feature representation of the sentence.\"\"\"\n",
    "#     vocabulary_set, word_to_ix = build_vocabulary(sentences, vocabulary_size)\n",
    "#     \n",
    "#     def bow_features(sentence):\n",
    "#         bow_vector = np.zeros(len(vocabulary_set), dtype=\"uint8\")\n",
    "#         \n",
    "#         for ix in preprocess(sentence, word_to_ix, vocabulary_set):\n",
    "#             bow_vector[ix] += 1\n",
    "#         return bow_vector\n",
    "#     \n",
    "#     return bow_features\n",
    "#\n",
    "#\n",
    "# Create a function to extract features for sample of the given dataset\n",
    "# bow_representation = feature_function(positives+negatives, vocabulary_size=200)\n",
    "#\n",
    "# This can then be used like this...\n",
    "# bow_representation('my new sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The quick brown fox jumped over the lazy dog. The dog was lazy and therefore didn't care about it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_set, word_to_idx = build_vocabulary(sentences, 3, \"<UNK>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0, 'The': 1, 'lazy': 2, 'dog': 3}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>', 'The', 'dog', 'lazy'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  2,  2,  2], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(sentences[0], vocabulary_set, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [13 , 1 , 2 , 1 , 0,  0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_labels = [1] * len(positives) + [-1] * len(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_data = positives + negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [bag_of_words(s, vocabulary_set, word_to_idx) for s in nlp_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor classifiers\n",
    "\n",
    "One common strategy: For a new sample $d$, find the closest known sample $d_j$, the so-called \"nearest neighbor\" (NN) of $d$, and classify $d$ as $l_j$ (i.e. the same class as the nearest neighbor).\n",
    "\n",
    "#### Formal description\n",
    "\n",
    "$$f(d) = l_j,\\quad j := \\text{argmin}_i\\, \\text{distance}(d,d_i)$$\n",
    "where $\\text{distance}(d,d_i)$ describes the distance between $d$ and $d_i$ according to some distance metric.\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "The library `sklearn` has a nearest neighbor classifier implemented in the `neighbors` package. Let's see how to use it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(processed_data, nlp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Wow this was awesome\"\n",
    "bow_sentence = bag_of_words(sentence, vocabulary_set, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"Wow this was very bad horrible terrible\"\n",
    "bow_sentence2 = bag_of_words(sentence2, vocabulary_set, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([bow_sentence, bow_sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'This movie was super bad',\n",
    "    'The movie was horrible and should not be ever watched again',\n",
    "    'I did not like the movie',\n",
    "    'Wow this was very bad horrible terrible',\n",
    "    \n",
    "    'This was an awesome movie',\n",
    "    'This was a great movie',\n",
    "    'I do not know if I will ever see a better movie in my life',\n",
    "    'Wow this was awesome'\n",
    "]\n",
    "\n",
    "test_labels = [-1, -1, -1, -1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sents = [bag_of_words(s, vocabulary_set, word_to_idx) for s in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  1,  1,  1,  1, -1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(processed_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(processed_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = np.array([-1, -1, -1, -1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True,  True,  True, False])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = predicted == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comparison[comparison == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(comparison[comparison == True]) / len(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The quick brown fox jumped over the lazy dog. The dog was lazy and therefore didn't care about it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_set, word_to_idx = build_vocabulary(sentences, 3, '<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>', 'The', 'dog', 'lazy'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  2,  2,  2], dtype=uint8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(sentences[0], vocabulary_set, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
